{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenTextSplitter\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Peer Review: [Teddy Lee](https://github.com/teddylee777)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Language models can only process a limited number of tokens (meaningful units of text) at once. This tutorial explores various methodes for splitting text into manageable chunks based on tokenization, ensuring capatibility with these limitations.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Example Usage of Tiktoken](#example-usage-of-tiktoken)\n",
    "- [Example Usage of TokenTextSplitter](#example-usage-of-tokentextsplitter)\n",
    "- [Example Usage of SentenceTransformers](#example-usage-of-sentencetransformers)\n",
    "- [Splitting Text with NLTK](#splitting-text-with-nltk)\n",
    "- [Splitting Text with spaCy](#splitting-text-with-spacy)\n",
    "- [Using KoNLPy for Korean NLP](#using-konlpy-for-korean-nlp)\n",
    "- [Basic Usage of Hugging Face tokenizers](#basic-usage-of-hugging-face-tokenizers)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain: How to split text by tokens](https://python.langchain.com/docs/how_to/split_by_token/)\n",
    "- [Langchain TokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
    "\n",
    "**[Note]**\n",
    "- The `langchain-opentutorial` is a bundle of easy-to-use environment setup guidance, useful functions and utilities for tutorials. \n",
    "- Check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"tiktoken\",\n",
    "        \"spacy\",\n",
    "        \"sentence-transformers\",\n",
    "        \"nltk\",\n",
    "        \"konlpy\",\n",
    "    ],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"TokenTextSplitter\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can set and load `OPENAI_API_KEY` from a `.env` file. \n",
    "\n",
    "**[Note]** This is only necessary if you haven't already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tiktoken for Text Splitting\n",
    "\n",
    "`tiktoken` is a fast BPE (Byte Pair Encoding) tokenizer developed by OpenAI. Here's an example demonstrating its use with a text splitter:\n",
    "\n",
    "1. Open the text file `appendix-keywords.txt` and read its contents. Store this text in a variable named `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display some of the content read from the `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to unders\n"
     ]
    }
   ],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `CharacterTextSplitter` with `tiktoken`:\n",
    "\n",
    "1. Initialize a text splitter using the `from_tiktoken_encoder` method. This method leverages the `tiktoken` encoder for measurement and merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # Set the chunk size to 300.\n",
    "    chunk_size=300,\n",
    "    # Ensure there is no overlap between chunks.\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the number of resulting text chunks after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))  # Output the number of divided chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print the first element of the `texts` list, which holds the split chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to understand and process the text.\n",
      "Example: Represent the word “apple” as a vector such as [0.65, -0.23, 0.17].\n",
      "Related keywords: natural language processing, vectorization, deep learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token is a breakup of text into smaller units. These can typically be words, sentences, or phrases.\n",
      "Example: Split the sentence “I am going to school” into “I am”, “to school”, and “going”.\n",
      "Associated keywords: tokenization, natural language processing, parsing\n",
      "\n",
      "Tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Print the first element of the texts list.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- When using `CharacterTextSplitter.from_tiktoken_encoder`, the text is split primarily by the `CharacterTextSplitter`. The `tiktoken` tokenizer is used for measuring and merging the divided text. This might lead to chunks exceeding the token size intended for the language model.\n",
    "- Consider `RecursiveCharacterTextSplitter.from_tiktoken_encoder` or directly loading the `tiktoken` splitter, for stricter control and ensuring each split adheres to the language model's token limit. If a split text exceeds this size, it is recursively divided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of TokenTextSplitter\n",
    "\n",
    "This section will cover using the `TokenTextSplitter` class to split text into chunks based on tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to understand and process the text.\n",
      "Example: Represent the word “apple” as a vector such as [0.65, -0.23, 0.17].\n",
      "Related keywords: natural language processing, vectorization, deep learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token is a breakup of text into smaller units. These can typically be words, sentences, or phrases.\n",
      "Example: Split the sentence “I am going to school\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 10.\n",
    "    chunk_overlap=0,  # Set the overlap between chunks to 0.\n",
    ")\n",
    "\n",
    "# Split the state_of_the_union text into chunks.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first chunk of the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of SentenceTransformers\n",
    "\n",
    "`SentenceTransformersTokenTextSplitter` is a specialized splitter designed for `sentence-transformer` models. It automatically splits text into chunks that fit within the token window of the sentence-transformer model being used.\n",
    "\n",
    "Steps:\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create a sentence splitter and set the overlap between chunks to 0.\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the number of tokens (excluding start and stop tokens) in the `file` variable, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n"
     ]
    }
   ],
   "source": [
    "count_start_and_stop_tokens = 2  # Set the number of start and stop tokens to 2.\n",
    "\n",
    "# Subtract the count of start and stop tokens from the total number of tokens in the text.\n",
    "text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # Print the calculated number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utilize the `splitter.split_text()` function to split the text stored in `text_to_split` into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=file)  # Split the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print the first chunk using `print(text_chunks[1])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ete, and more data. example : select * from users where age > 18 ; looks up information about users who are 18 years old or older. associated keywords : database, query, data management, data management csv definition : csv ( comma - separated values ) is a file format for storing data, where each data value is separated by a comma. it is used for simple storage and exchange of tabular data. example : a csv file with the headers name, age, and occupation might contain data such as hong gil - dong, 30, developer. related keywords : data format, file processing, data exchange json definition : json ( javascript object notation ) is a lightweight data interchange format that represents data objects using text that is readable to both humans and machines. example : { “ name ” : “ honggildong ”, ‘ age ’ : 30, “ occupation ” : “ developer \" } is data in json format. related keywords : data exchange, web development, apis transformer definition : transformers are a type of deep learning model used in natural language processing, mainly for translation, summarization, text generation, etc. it is based on the attention mechanism. example : google translator uses transformer models to perform translations between different languages. related keywords : deep learning, natural language processing, attention huggingface definition : huggingface is a library that provides a variety of pre - trained models and tools for natural language processing. it helps researchers and developers to easily perform nlp tasks. example : you can use huggingface's transformers library to perform tasks such as sentiment analysis, text generation, and more. related keywords : natural language processing, deep learning, libraries digital transformation definition : digital transformation is the process of leveraging technology to transform a company's services, culture, and operations. it focuses on improving business models and increasing\n"
     ]
    }
   ],
   "source": [
    "# Print the 0th chunk.\n",
    "print(text_chunks[1])  # Print the second chunk from the divided text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a Python library for natural language processing (NLP) tasks. It supports various NLP tasks like text preprocessing, tokenization, morphological analysis, and part-of-speech tagging.\n",
    "\n",
    "Here's how to use NLTK tokenizers for text splitting, offering an alternative to splitting by newlines (`\\n\\n`).\n",
    "- Splitting method: NLTK tokenizer\n",
    "- The chunk size is determined by the number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using NLTK, you need to download the necessary data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/teddy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading `punkt_tab` enables NLTK to tokenize text into words or sentences for multiple languages, including English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a text splitter using the `NLTKTextSplitter` class.\n",
    "3. Set the `chunk_size` parameter to 200 (or any desired value) to control the mazimum chunk size in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 200.\n",
    "    chunk_overlap=0,  # Set the overlap between chunks to 0.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utilize the `split_text` method of the `text_splitter` object to split the text stored in `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 240, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 231, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 213, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 213, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 224, which is longer than the specified 200\n",
      "Created a chunk of size 218, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format.\n",
      "\n",
      "It is used for search, classification, and other data analysis tasks.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with spaCy\n",
    "\n",
    "spaCy is an open-source library for advanced NLP, written in Python and Cython.\n",
    "\n",
    "Like NLTK, spaCy also provides an alternative to basic newline splitting (`\\n\\n`).\n",
    "- Splitting method: spaCy's tokenizer\n",
    "- The chunk size is measured by the number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split text with spaCy, you need to download the `en_core_web_sm` spaCy model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "\n",
    "\n",
    "\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a text splitter using the `SpacyTextSplitter` class.\n",
    "3. Set the `chunk_size` parameter to 200 (or any desired value) to control the mazimum chunk size in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# Ignore  warning messages.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create the SpacyTextSplitter.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 200.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the `split_text` method of the `text_splitter` object to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 241, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 231, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 218, which is longer than the specified 200\n",
      "Created a chunk of size 230, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format.\n",
      "\n",
      "It is used for search, classification, and other data analysis tasks.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KoNLPy for Korean NLP\n",
    "\n",
    "As mentioned in the [LangChain's How-to guides](https://python.langchain.com/docs/how_to/split_by_token/#konlpy), KoNLPy offers a dedicated text splitter for Korean text processing with useful features for morphological analysis, part-of-speech tagging, and syntactic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Since it is an example of processing Korean language, we need to log Korean text to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords-korean.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a text splitter using the `KonlpyTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# Create a text splitter object using KonlpyTextSplitter.\n",
    "text_splitter = KonlpyTextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the `text_splitter` to split `the file` content into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(file)  # Split the file content into sentences.\n",
    "print(texts[0])  # Print the first sentence from the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of Hugging Face tokenizers\n",
    "\n",
    "Hugging Face provides various tokenizers.\n",
    "\n",
    "This tutorial demonstrates calculating the token length of a text using one of Hugging Face's tokenizers, `GPT2TokenizerFast`.\n",
    "- Splitting method: Hugging Face's `GPT2TokenizerFast`\n",
    "- The chunk size is determined by the number of characters\n",
    "\n",
    "**[Note]**\n",
    "- The chunk size is based on the number of tokens calculated by the Hugging Face tokenizer.\n",
    "- A `tokenizer` object is created using the `GPT2TokenizerFast` class.\n",
    "\n",
    "Call `from_pretrained` method to load the pre-trained `gpt2` tokenizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load the GPT-2 tokenizer.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a text splitter using `from_huggingface_tokenizer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    # Use the Hugging Face tokenizer to create a CharacterTextSplitter object.\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check the split result of the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is used to preprocess data in natural language processing.\n",
      "Example: Split the sentence “I love programming.” into [“I”, “love”, “programming”, “.”].\n",
      "Associated keywords: tokenization, natural language processing, parsing\n",
      "\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "SQL\n",
      "\n",
      "Definition: SQL(Structured Query Language) is a programming language for managing data in a database. You can query, modify, insert, delete, and more data.\n",
      "Example: SELECT * FROM users WHERE age > 18; looks up information about users who are 18 years old or older.\n",
      "Associated keywords: database, query, data management, data management\n",
      "\n",
      "CSV\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])  # Print the first element of the texts list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
